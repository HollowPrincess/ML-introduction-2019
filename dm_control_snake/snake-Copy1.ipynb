{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(300, input_dim = 4, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(5, activation = \"relu\"))\n",
    "#model.add(tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1, 1)))\n",
    "model.compile(loss='mse', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    list1 = [i.tolist() for i in list(obs.values())]\n",
    "    list2 = np.array([])\n",
    "    for sublist in list1:\n",
    "        list2 = np.append(list2, sublist)\n",
    "    return list2.reshape([1,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1801.00690.pdf\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-3, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, action_spec):\n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "        }\n",
    "        self.batch_size=64\n",
    "        self.discount_rate=0.99\n",
    "        self.tau=0.001\n",
    "        self.action_dim = len(action_spec.minimum) #action_spec.shape\n",
    "        self.action_spec = action_spec\n",
    "        self.action_bound = 1\n",
    "        self.input_size = 25\n",
    "        self.noise = 0\n",
    "        \n",
    "        self.critic_opt = tf.optimizers.Adam(1e-4)\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-4, clipvalue=1)\n",
    "        \n",
    "\n",
    "        # create critic model:\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(400, activation = 'relu')(input_obs)\n",
    "        tmp1 = tf.keras.layers.Dense(300, activation = 'relu')(h)\n",
    "        action_abs = tf.keras.layers.Dense(300, activation = 'relu')(input_actions)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        pred = tf.keras.layers.Dense(1, activation = 'relu')(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)   \n",
    "        model.compile(loss='mse', optimizer=self.critic_opt)\n",
    "        self.critic_model = model\n",
    "\n",
    "        \"\"\"\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(64, activation = 'relu')(input_obs)\n",
    "        #h = BatchNormalization()(h)\n",
    "        tmp1 = tf.keras.layers.Dense(64)(h)\n",
    "        action_abs = tf.keras.layers.Dense(64, activation = 'relu')(input_actions)\n",
    "        #action_abs = Activation('relu')(action_abs)\n",
    "        #action_abs = BatchNormalization()(action_abs)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        #h = Dense(64)(h)\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        #h = BatchNormalization()(h)\n",
    "        pred = tf.keras.layers.Dense(1, kernel_initializer='random_uniform')(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        self.critic_model = model\n",
    "        \"\"\"\n",
    "        # create actor model:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(300, input_dim = self.input_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"relu\"))\n",
    "        #model.add(tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1, 1)))\n",
    "        model.compile(loss=tf.math.reduce_mean(), optimizer=self.actor_opt)\n",
    "        self.actor_model = model\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(400, input_dim = self.input_size))#, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(300))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\"))\n",
    "        model.compile(loss='mse', optimizer=\"Adam\")\n",
    "        self.actor_model = model\n",
    "        \"\"\"\n",
    "        # target models:\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    " \n",
    "    def sample_action(self, obs):\n",
    "        action = self.actor_model.predict(obs)\n",
    "        action = action + self.noise()\n",
    "        #0.3*np.random.normal(0, 1, self.action_spec.shape) #self.noise()# \n",
    "        action = np.clip(action, -1, 1)\n",
    "        return action\n",
    "    \n",
    "    def init_noise(self):\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(mu = np.zeros(self.action_dim))\n",
    "    \n",
    "    def store_info(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1e6:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-int(0.9e6):]\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else:            \n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)        \n",
    "        \n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size:\n",
    "            # get batch\n",
    "            idxs = np.random.randint(self.memory[\"done\"].shape[0], size=self.batch_size)\n",
    "            batch = {\n",
    "                \"state\": np.squeeze(self.memory[\"state\"][idxs]), \n",
    "                \"action\": np.squeeze(self.memory[\"action\"][idxs]), \n",
    "                \"reward\": self.memory[\"reward\"][idxs], \n",
    "                \"new_state\": np.squeeze(self.memory[\"new_state\"][idxs]), \n",
    "                \"done\": self.memory[\"done\"][idxs]\n",
    "            }\n",
    "            \n",
    "            # replay:\n",
    "            #target_q = self.target_critic_model.predict_on_batch({\"state\":batch[\"new_state\"], \n",
    "            #                                \"action\": self.target_actor_model.predict_on_batch(batch[\"new_state\"])})\n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.target_actor_model(batch[\"new_state\"])})\n",
    "\n",
    "            #y = batch[\"reward\"].reshape(self.batch_size,1) + self.discount_rate*target_q\n",
    "            y = batch[\"reward\"].reshape(self.batch_size,1) + np.multiply(target_q, \n",
    "                                                                        (self.discount_rate*(1-batch[\"done\"])\n",
    "                                                                        ).reshape(self.batch_size,1)\n",
    "                                                                        ) \n",
    "            \n",
    "            # update critic\n",
    "            #self.critic_model.train_on_batch({\"state\": batch[\"state\"], \"action\": batch[\"action\"]}, y)  \n",
    "            # trainable weights or variables?\n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": batch[\"action\"]})\n",
    "                td_error = tf.losses.mean_squared_error(y, q)\n",
    "            critic_grads = tape.gradient(td_error, self.critic_model.trainable_weights)\n",
    "            self.critic_opt.apply_gradients(zip(critic_grads, self.critic_model.trainable_weights))\n",
    "                \n",
    "            \n",
    "            # update actor policy\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "            \n",
    "            \n",
    "            #actions_pred = self.actor_model.predict_on_batch(batch[\"state\"])\n",
    "            #critic_pred = self.critic_model.predict_on_batch({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "            #self.actor_model.train_on_batch(batch[\"state\"], critic_pred)    \n",
    "    \n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "            \"\"\"            \n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                tape.watch(actions_pred)\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "            dq_da = tape.gradient(q, actions_pred)\n",
    "            \n",
    "            self.actor_model.train_on_batch(batch[\"state\"], dq_da)\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                theta = self.actor_model.trainable_variables\n",
    "            da_dtheta = tape.gradient(actions_pred, theta, output_gradients=-dq_da)\n",
    "            \n",
    "            self.actor_opt.apply_gradients(zip(da_dtheta, \n",
    "                                               self.actor_model.trainable_variables))\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            # update networks\n",
    "            self.critic_model.set_weights(self.tau * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          (1.0-self.tau)*np.array(self.critic_model.get_weights()))\n",
    "            self.actor_model.set_weights(self.tau * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         (1.0-self.tau)*np.array(self.actor_model.get_weights()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent):\n",
    "    agent.critic_model.save('critic_model.h5')\n",
    "    agent.actor_model.save('actor_model.h5')\n",
    "    agent.target_critic_model.save('target_critic_model.h5')\n",
    "    agent.target_actor_model.save('target_actor_model.h5')\n",
    "    \n",
    "def train_model(env, n_iterations, batch_size, discount_rate):\n",
    "    scores = np.array([])\n",
    "    last_rewards = np.array([])\n",
    "    first_rewards = np.array([])\n",
    "    action_spec = env.action_spec()\n",
    "    agent = Agent(action_spec)\n",
    "    start = time.time()\n",
    "    for iteration in range(n_iterations):\n",
    "        agent.init_noise()\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            save_models(agent)\n",
    "\n",
    "        time_step = env.reset()  \n",
    "        obs = convert_observation(time_step.observation)\n",
    "\n",
    "        game_score = 0\n",
    "        while not time_step.last():       \n",
    "            # get action:\n",
    "            action = agent.sample_action(obs)\n",
    "            # make action:\n",
    "            time_step = env.step(action[0])\n",
    "            new_obs = convert_observation(time_step.observation)            \n",
    "            # update history:\n",
    "            game_score += time_step.reward \n",
    "            agent.store_info(obs, new_obs, action, time_step.reward, 0)\n",
    "            obs = new_obs\n",
    "            # experience replay:\n",
    "            agent.experience_replay()\n",
    "            if len(first_rewards)==len(last_rewards):\n",
    "                first_rewards = np.append(first_rewards, time_step.reward)\n",
    "\n",
    "        new_obs = convert_observation(time_step.observation)   \n",
    "        agent.store_info(obs, new_obs, action, time_step.reward, 1)\n",
    "        agent.experience_replay()\n",
    "        game_score += time_step.reward \n",
    "        \n",
    "        scores = np.append(scores, game_score)\n",
    "        last_rewards = np.append(last_rewards, time_step.reward)\n",
    "        print(\"Iteration: {}; score: {:10.3f}; last_reward: {:10.3f}; first_reward: {:10.3f}\".format(\n",
    "            iteration, game_score, time_step.reward, first_rewards[-1]))\n",
    "        print(\"{:10.3f} minutes remaining\".format((time.time()-start)/60))\n",
    "\n",
    "    save_models(agent)\n",
    "    return scores, last_rewards, first_rewards, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; score:      8.035; last_reward:      0.007; first_reward:      0.009\n",
      "     1.069 minutes remaining\n",
      "Iteration: 1; score:      7.329; last_reward:      0.007; first_reward:      0.008\n",
      "     2.134 minutes remaining\n",
      "Iteration: 2; score:     25.099; last_reward:      0.025; first_reward:      0.023\n",
      "     3.195 minutes remaining\n",
      "Iteration: 3; score:      6.535; last_reward:      0.007; first_reward:      0.007\n",
      "     4.283 minutes remaining\n",
      "Iteration: 4; score:     34.114; last_reward:      0.035; first_reward:      0.022\n",
      "     5.337 minutes remaining\n",
      "Iteration: 5; score:     27.686; last_reward:      0.028; first_reward:      0.017\n",
      "     6.406 minutes remaining\n",
      "Iteration: 6; score:     25.710; last_reward:      0.026; first_reward:      0.018\n",
      "     7.482 minutes remaining\n",
      "Iteration: 7; score:     61.837; last_reward:      0.060; first_reward:      0.113\n",
      "     8.574 minutes remaining\n",
      "Iteration: 8; score:     57.781; last_reward:      0.059; first_reward:      0.035\n",
      "     9.660 minutes remaining\n",
      "Iteration: 9; score:    596.605; last_reward:      0.626; first_reward:      0.238\n",
      "    10.753 minutes remaining\n",
      "Iteration: 10; score:    826.700; last_reward:      0.762; first_reward:      0.809\n",
      "    11.897 minutes remaining\n",
      "Iteration: 11; score:    122.825; last_reward:      0.127; first_reward:      0.056\n",
      "    13.021 minutes remaining\n",
      "Iteration: 12; score:      4.924; last_reward:      0.005; first_reward:      0.005\n",
      "    14.246 minutes remaining\n",
      "Iteration: 13; score:    293.690; last_reward:      0.252; first_reward:      0.964\n",
      "    15.357 minutes remaining\n",
      "Iteration: 14; score:     10.612; last_reward:      0.011; first_reward:      0.012\n",
      "    16.470 minutes remaining\n",
      "Iteration: 15; score:      5.911; last_reward:      0.006; first_reward:      0.008\n",
      "    17.590 minutes remaining\n",
      "Iteration: 16; score:     18.515; last_reward:      0.018; first_reward:      0.028\n",
      "    18.742 minutes remaining\n",
      "Iteration: 17; score:     10.578; last_reward:      0.011; first_reward:      0.008\n",
      "    20.004 minutes remaining\n",
      "Iteration: 18; score:      9.862; last_reward:      0.010; first_reward:      0.009\n",
      "    21.170 minutes remaining\n",
      "Iteration: 19; score:    258.409; last_reward:      0.207; first_reward:      0.983\n",
      "    22.326 minutes remaining\n",
      "Iteration: 20; score:     48.136; last_reward:      0.043; first_reward:      0.051\n",
      "    23.624 minutes remaining\n",
      "Iteration: 21; score:     13.356; last_reward:      0.014; first_reward:      0.010\n",
      "    24.793 minutes remaining\n",
      "Iteration: 22; score:      9.355; last_reward:      0.009; first_reward:      0.013\n",
      "    25.955 minutes remaining\n",
      "Iteration: 23; score:      9.281; last_reward:      0.009; first_reward:      0.013\n",
      "    27.139 minutes remaining\n",
      "Iteration: 24; score:      3.565; last_reward:      0.003; first_reward:      0.004\n",
      "    28.321 minutes remaining\n",
      "Iteration: 25; score:      5.307; last_reward:      0.005; first_reward:      0.007\n",
      "    29.510 minutes remaining\n",
      "Iteration: 26; score:     14.025; last_reward:      0.015; first_reward:      0.010\n",
      "    30.693 minutes remaining\n",
      "Iteration: 27; score:      7.954; last_reward:      0.008; first_reward:      0.006\n",
      "    31.878 minutes remaining\n",
      "Iteration: 28; score:     16.766; last_reward:      0.017; first_reward:      0.011\n",
      "    33.096 minutes remaining\n",
      "Iteration: 29; score:      4.099; last_reward:      0.004; first_reward:      0.005\n",
      "    34.289 minutes remaining\n",
      "Iteration: 30; score:     51.138; last_reward:      0.050; first_reward:      0.034\n",
      "    35.563 minutes remaining\n",
      "Iteration: 31; score:     38.450; last_reward:      0.041; first_reward:      0.021\n",
      "    36.834 minutes remaining\n",
      "Iteration: 32; score:    944.499; last_reward:      1.000; first_reward:      0.370\n",
      "    38.102 minutes remaining\n",
      "Iteration: 33; score:    995.573; last_reward:      1.000; first_reward:      1.000\n",
      "    39.380 minutes remaining\n",
      "Iteration: 34; score:      4.860; last_reward:      0.005; first_reward:      0.005\n",
      "    40.599 minutes remaining\n",
      "Iteration: 35; score:      6.862; last_reward:      0.007; first_reward:      0.007\n",
      "    41.823 minutes remaining\n",
      "Iteration: 36; score:      6.976; last_reward:      0.007; first_reward:      0.007\n",
      "    43.064 minutes remaining\n",
      "Iteration: 37; score:      5.438; last_reward:      0.005; first_reward:      0.006\n",
      "    44.373 minutes remaining\n",
      "Iteration: 38; score:     31.140; last_reward:      0.032; first_reward:      0.027\n",
      "    45.674 minutes remaining\n",
      "Iteration: 39; score:    959.835; last_reward:      0.931; first_reward:      1.000\n",
      "    47.077 minutes remaining\n",
      "Iteration: 40; score:    321.408; last_reward:      0.185; first_reward:      0.875\n",
      "    48.486 minutes remaining\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9a14a0b55e4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maction_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-da5fc9b02381>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(env, n_iterations, batch_size, discount_rate)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;31m# experience replay:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mfirst_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7fe1a99ec506>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[0mtd_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mcritic_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    442\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m           kwargs={\"name\": name})\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1947\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1948\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1949\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1951\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1954\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   1955\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1956\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1957\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    486\u001b[0m           update_ops.extend(\n\u001b[0;32m    487\u001b[0m               distribution.extended.update(\n\u001b[1;32m--> 488\u001b[1;33m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   1541\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1543\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2172\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2173\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2174\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2176\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   2178\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2180\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2181\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    468\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"apply_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apply_state\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[0;32m    205\u001b[0m           \u001b[0mcoefficients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epsilon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m           use_locking=self._use_locking)\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vhat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\training\\gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[1;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[0;32m   1409\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m         \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_locking\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_nesterov\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1411\u001b[1;33m         use_nesterov)\n\u001b[0m\u001b[0;32m   1412\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iterations = 400\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")\n",
    "action_spec = env.action_spec()\n",
    "\n",
    "scores, last_rewards, first_rewards, agent = train_model(env, n_iterations, batch_size=64, discount_rate=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_arr = np.array((last_rewards[-100:] - first_rewards[-100:])>0, dtype = int)\n",
    "plt.figure(figsize=(10,7))\n",
    "better_results = np.sum(better_arr)\n",
    "plt.bar([0, 1],[len(better_arr)-better_results, better_results], 0.3)\n",
    "plt.xticks([0,1],labels = [\"False\", \"True\"])\n",
    "plt.title(\"Learning curves: is the last point is closer to the target than the first point\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xlabel(\"Is closer\")\n",
    "plt.savefig(\"closer_points.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "x=list(range(len(scores)))\n",
    "plt.plot(x, scores)\n",
    "\n",
    "plt.title(\"Learning curves: score per iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.savefig(\"learning_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_games = 100\n",
    "\n",
    "test_scores = []\n",
    "test_last_rewards = []\n",
    "for game_index in range(test_games):\n",
    "    score = 0\n",
    "    time_step = env.reset()  \n",
    "    obs = convert_observation(time_step.observation)\n",
    "    while not time_step.last(): \n",
    "        action = agent.target_actor_model.predict(obs)\n",
    "        time_step = env.step(action[0])\n",
    "        obs = convert_observation(time_step.observation)\n",
    "        score += time_step.reward\n",
    "\n",
    "    test_scores.append(score)\n",
    "    test_last_rewards.append(time_step.reward)\n",
    "\n",
    "print(\"Average reward on test 100 games: \", np.mean(test_scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Reward on test 100 games')\n",
    "ax.boxplot(test_scores,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward\")\n",
    "plt.savefig(\"rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Rewards on the last step on test 100 games')\n",
    "ax.boxplot(test_last_rewards,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward on the last step\")\n",
    "plt.savefig(\"last_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
