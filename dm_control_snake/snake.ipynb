{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    list1 = [i.tolist() for i in list(obs.values())]\n",
    "    list2 = np.array([])\n",
    "    for sublist in list1:\n",
    "        list2 = np.append(list2, sublist)\n",
    "    return list2.reshape([1,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and models from : https://arxiv.org/pdf/1801.00690.pdf\n",
    "# https://spinningup.openai.com/en/latest/algorithms/ddpg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "class Agent:\n",
    "    def actor_loss(self,y_true,y_pred):\n",
    "        return - tf.reduce_mean(y_true)\n",
    "    \n",
    "    def __init__(self, action_spec):\n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "        }\n",
    "        self.batch_size=64\n",
    "        self.discount_rate=0.99\n",
    "        self.tau=0.001\n",
    "        self.action_dim = len(action_spec.minimum)\n",
    "        self.action_spec = action_spec\n",
    "        self.action_bound = 1.0\n",
    "        self.input_size = 25  \n",
    "        self.noise_scale = 1.0 #0.3\n",
    "        self.action_range = 1.0\n",
    "        \n",
    "        self.critic_opt = tf.optimizers.Adam(1e-4)\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-4)#, clipvalue=1.0)\n",
    "        \n",
    "\n",
    "        # create critic model:\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(400, activation = 'relu')(input_obs)\n",
    "        tmp1 = tf.keras.layers.Dense(300, activation = 'relu')(h)\n",
    "        action_abs = tf.keras.layers.Dense(300, activation = 'relu')(input_actions)\n",
    "        #h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Concatenate()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        pred = tf.keras.layers.Dense(1, activation = 'sigmoid', \n",
    "                                     kernel_initializer=tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3))(h) \n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)   \n",
    "        model.compile(loss='mse', optimizer=self.critic_opt)\n",
    "        self.critic_model = model\n",
    "\n",
    "        # create actor model:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(400, input_dim = self.input_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(300, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\", \n",
    "                                        kernel_initializer=tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)))\n",
    "        model.compile(loss = self.actor_loss, optimizer=self.actor_opt)\n",
    "        self.actor_model = model\n",
    "\n",
    "        # target models:\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    " \n",
    "    def sample_action(self, obs):\n",
    "        action = self.actor_model.predict(obs)[0]*self.action_range\n",
    "        action = action + self.noise_scale*np.random.normal(size = self.action_dim)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        return action\n",
    "           \n",
    "    \n",
    "    def store_info(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1e6:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-int(0.9e6):]\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else:            \n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)        \n",
    "        \n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size:\n",
    "            # get batch\n",
    "            idxs = np.random.randint(self.memory[\"done\"].shape[0], size=self.batch_size)\n",
    "            batch = {\n",
    "                \"state\": np.squeeze(self.memory[\"state\"][idxs]), \n",
    "                \"action\": np.squeeze(self.memory[\"action\"][idxs]), \n",
    "                \"reward\": self.memory[\"reward\"][idxs], \n",
    "                \"new_state\": np.squeeze(self.memory[\"new_state\"][idxs]), \n",
    "                \"done\": self.memory[\"done\"][idxs]\n",
    "            }\n",
    "            \n",
    "            # replay:\n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.action_range*self.target_actor_model(batch[\"new_state\"])})\n",
    "\n",
    "            y = batch[\"reward\"] + np.multiply(tf.reshape(target_q, (self.batch_size,)), \n",
    "                                                                        (self.discount_rate*(1-batch[\"done\"])\n",
    "                                                                        ))\n",
    "\n",
    "            # update critic\n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": batch[\"action\"]}, training = True)\n",
    "                td_error = tf.losses.mean_squared_error(y, \n",
    "                                                        tf.reshape(q,(self.batch_size,)))\n",
    "            critic_grads = tape.gradient(td_error, self.critic_model.trainable_weights)            \n",
    "            self.critic_opt.apply_gradients(zip(critic_grads, self.critic_model.trainable_weights))\n",
    "\n",
    "            \n",
    "            # update actor policy\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"], training = True)\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred}, training = False)\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "\n",
    "                        \n",
    "            # update networks\n",
    "            self.target_critic_model.set_weights((1.0-self.tau) * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          self.tau*np.array(self.critic_model.get_weights()))\n",
    "            self.target_actor_model.set_weights((1.0-self.tau) * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         self.tau*np.array(self.actor_model.get_weights()))\n",
    "            \n",
    "            self.noise_scale*=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent):\n",
    "    agent.critic_model.save('critic_model.h5')\n",
    "    agent.actor_model.save('actor_model.h5')\n",
    "    agent.target_critic_model.save('target_critic_model.h5')\n",
    "    agent.target_actor_model.save('target_actor_model.h5')\n",
    "    \n",
    "def train_model(env, n_iterations, batch_size, discount_rate):\n",
    "    scores = np.array([])\n",
    "    last_rewards = np.array([])\n",
    "    first_rewards = np.array([])\n",
    "    action_spec = env.action_spec()\n",
    "    agent = Agent(action_spec)\n",
    "    start = time.time()\n",
    "    for iteration in range(n_iterations):       \n",
    "        if iteration % 25 == 0:\n",
    "            save_models(agent)\n",
    "            print(\"{:10.3f} minutes remaining\".format((time.time()-start)/60))\n",
    "        if (iteration+1) % 25 == 0:\n",
    "            plt.figure(figsize=(15,7))\n",
    "            plt.grid()\n",
    "            x=list(range(len(scores)))\n",
    "            plt.plot(x, scores)\n",
    "\n",
    "            plt.title(\"Learning curves: score per iteration\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Iteration number\")\n",
    "            plt.show()\n",
    "\n",
    "        time_step = env.reset()  \n",
    "        obs = convert_observation(time_step.observation)\n",
    "\n",
    "        game_score = 0\n",
    "        while not time_step.last():       \n",
    "            # get action:\n",
    "            action = agent.sample_action(obs)\n",
    "            # make action:\n",
    "            time_step = env.step(action)\n",
    "            new_obs = convert_observation(time_step.observation)            \n",
    "            # update history:\n",
    "            game_score += time_step.reward \n",
    "            agent.store_info(obs, new_obs, action, time_step.reward, 0)\n",
    "            obs = new_obs\n",
    "            # experience replay:\n",
    "            agent.experience_replay()\n",
    "            if len(first_rewards)==len(last_rewards):\n",
    "                first_rewards = np.append(first_rewards, time_step.reward)\n",
    "\n",
    "        new_obs = convert_observation(time_step.observation)   \n",
    "        agent.store_info(obs, new_obs, action, time_step.reward, 1)\n",
    "        agent.experience_replay()\n",
    "        game_score += time_step.reward \n",
    "        \n",
    "        scores = np.append(scores, game_score)\n",
    "        last_rewards = np.append(last_rewards, time_step.reward)\n",
    "        print(\"Iteration: {};   score: {:10.3f}; last_reward: {:10.3f}; first_reward: {:10.3f}\".format(\n",
    "            iteration, game_score, time_step.reward, first_rewards[-1]))\n",
    "\n",
    "    save_models(agent)\n",
    "    return scores, last_rewards, first_rewards, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.008 minutes remaining\n",
      "Iteration: 0;   score:    794.597; last_reward:      0.940; first_reward:      0.556\n",
      "Iteration: 1;   score:     16.560; last_reward:      0.016; first_reward:      0.015\n",
      "Iteration: 2;   score:     10.130; last_reward:      0.010; first_reward:      0.011\n",
      "Iteration: 3;   score:      7.061; last_reward:      0.007; first_reward:      0.007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a14c5f0c8fd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maction_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-cbf1bb720e80>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(env, n_iterations, batch_size, discount_rate)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# experience replay:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mfirst_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8d89e0d2b726>\u001b[0m in \u001b[0;36mexperience_replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 td_error = tf.losses.mean_squared_error(y, \n\u001b[0;32m    104\u001b[0m                                                         tf.reshape(q,(self.batch_size,)))\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mcritic_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1618\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstFirstOnly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1619\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1620\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_MatMulGradAgainstSecondOnly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1621\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[1;31m# No gradient skipping, so do the full gradient computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGradAgainstSecondOnly\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1599\u001b[0m   \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5603\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5604\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5605\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5606\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5607\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")\n",
    "action_spec = env.action_spec()\n",
    "\n",
    "scores, last_rewards, first_rewards, agent = train_model(env, n_iterations, batch_size=64, discount_rate=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_arr = np.array((last_rewards[-100:] - first_rewards[-100:])>0, dtype = int)\n",
    "plt.figure(figsize=(10,7))\n",
    "better_results = np.sum(better_arr)\n",
    "plt.bar([0, 1],[len(better_arr)-better_results, better_results], 0.3)\n",
    "plt.xticks([0,1],labels = [\"False\", \"True\"])\n",
    "plt.title(\"Learning curves: is the last point is closer to the target than the first point\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xlabel(\"Is closer\")\n",
    "plt.savefig(\"closer_points.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "x=list(range(len(scores)))\n",
    "plt.plot(x, scores)\n",
    "\n",
    "plt.title(\"Learning curves: score per iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.savefig(\"learning_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    print(i,\"Average learning score on 100 steps: \", np.mean(scores[(i-1)*100: i*100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_games = 100\n",
    "\n",
    "test_scores = []\n",
    "test_last_rewards = []\n",
    "for game_index in range(test_games):\n",
    "    score = 0\n",
    "    time_step = env.reset()  \n",
    "    obs = convert_observation(time_step.observation)\n",
    "    while not time_step.last(): \n",
    "        action = agent.target_actor_model.predict(obs)\n",
    "        time_step = env.step(action[0])\n",
    "        obs = convert_observation(time_step.observation)\n",
    "        score += time_step.reward\n",
    "\n",
    "    test_scores.append(score)\n",
    "    test_last_rewards.append(time_step.reward)\n",
    "\n",
    "print(\"Average reward on test 100 games: \", np.mean(test_scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Reward on test 100 games')\n",
    "ax.boxplot(test_scores,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward\")\n",
    "plt.savefig(\"rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Rewards on the last step on test 100 games')\n",
    "ax.boxplot(test_last_rewards,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward on the last step\")\n",
    "plt.savefig(\"last_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
