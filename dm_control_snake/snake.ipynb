{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    list1 = [i.tolist() for i in list(obs.values())]\n",
    "    list2 = np.array([])\n",
    "    for sublist in list1:\n",
    "        list2 = np.append(list2, sublist)\n",
    "    return list2.reshape([1,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and models from : https://arxiv.org/pdf/1801.00690.pdf\n",
    "# https://spinningup.openai.com/en/latest/algorithms/ddpg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running now:\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "class Agent:\n",
    "    def actor_loss(self,y_true,y_pred):\n",
    "        return - tf.reduce_mean(y_true)\n",
    "    \n",
    "    def __init__(self, action_spec):\n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "        }\n",
    "        self.batch_size=64\n",
    "        self.discount_rate=0.99\n",
    "        self.tau=0.001\n",
    "        self.action_dim = len(action_spec.minimum)\n",
    "        self.action_spec = action_spec\n",
    "        self.action_bound = 1.0\n",
    "        self.input_size = 25  \n",
    "        self.noise_scale = 0.3\n",
    "        self.action_range = 1.0\n",
    "        \n",
    "        self.critic_opt = tf.optimizers.Adam(1e-4)\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-4, clipvalue=1.0)\n",
    "        \n",
    "\n",
    "        # create critic model:\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(400, activation = 'relu')(input_obs)\n",
    "        tmp1 = tf.keras.layers.Dense(300, activation = 'relu')(h)\n",
    "        action_abs = tf.keras.layers.Dense(300, activation = 'relu')(input_actions)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        pred = tf.keras.layers.Dense(1, activation = 'sigmoid')(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)   \n",
    "        model.compile(loss='mse', optimizer=self.critic_opt)\n",
    "        self.critic_model = model\n",
    "\n",
    "        # create actor model:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(300, input_dim = self.input_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\"))\n",
    "        model.compile(loss = self.actor_loss, optimizer=self.actor_opt)\n",
    "        self.actor_model = model\n",
    "\n",
    "        # target models:\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    " \n",
    "    def sample_action(self, obs):\n",
    "        action = self.actor_model.predict(obs)[0]*self.action_range\n",
    "        action = action + self.noise_scale*np.random.normal(size = self.action_dim)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        return action\n",
    "           \n",
    "    \n",
    "    def store_info(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1e6:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-int(0.9e6):]\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else:            \n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)        \n",
    "        \n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size:\n",
    "            # get batch\n",
    "            idxs = np.random.randint(self.memory[\"done\"].shape[0], size=self.batch_size)\n",
    "            batch = {\n",
    "                \"state\": np.squeeze(self.memory[\"state\"][idxs]), \n",
    "                \"action\": np.squeeze(self.memory[\"action\"][idxs]), \n",
    "                \"reward\": self.memory[\"reward\"][idxs], \n",
    "                \"new_state\": np.squeeze(self.memory[\"new_state\"][idxs]), \n",
    "                \"done\": self.memory[\"done\"][idxs]\n",
    "            }\n",
    "            \n",
    "            # replay:\n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.action_range*self.target_actor_model(batch[\"new_state\"])})\n",
    "\n",
    "            y = batch[\"reward\"].reshape(self.batch_size,1) + np.multiply(target_q, \n",
    "                                                                        (self.discount_rate*(1-batch[\"done\"])\n",
    "                                                                        ).reshape(self.batch_size,1)\n",
    "                                                                        ) \n",
    "            \n",
    "            # update critic\n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": batch[\"action\"]})\n",
    "                td_error = tf.losses.mean_squared_error(y, q)\n",
    "            critic_grads = tape.gradient(td_error, self.critic_model.trainable_weights)\n",
    "            self.critic_opt.apply_gradients(zip(critic_grads, self.critic_model.trainable_weights))\n",
    "\n",
    "            \n",
    "            # update actor policy\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])*self.action_range\n",
    "                #tape.watch(actions_pred)\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "\n",
    "                        \n",
    "            # update networks\n",
    "            self.critic_model.set_weights(self.tau * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          (1.0-self.tau)*np.array(self.critic_model.get_weights()))\n",
    "            self.actor_model.set_weights(self.tau * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         (1.0-self.tau)*np.array(self.actor_model.get_weights()))\n",
    "            \n",
    "            self.noise_scale*=0.995\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new:\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "class Agent:\n",
    "    def actor_loss(self,y_true,y_pred):\n",
    "        return - tf.reduce_mean(y_true)\n",
    "    \n",
    "    def __init__(self, action_spec):\n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "        }\n",
    "        self.batch_size=64\n",
    "        self.discount_rate=0.99\n",
    "        self.tau=0.001\n",
    "        self.action_dim = len(action_spec.minimum)\n",
    "        self.action_spec = action_spec\n",
    "        self.action_bound = 1.0\n",
    "        self.input_size = 25  \n",
    "        self.noise_scale = 1.0 #0.3\n",
    "        self.action_range = 1.0\n",
    "        \n",
    "        self.critic_opt = tf.optimizers.Adam(1e-4)\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-4)#, clipvalue=1.0)\n",
    "        \n",
    "\n",
    "        # create critic model:\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(400, activation = 'relu')(input_obs)\n",
    "        tmp1 = tf.keras.layers.Dense(300, activation = 'relu')(h)\n",
    "        action_abs = tf.keras.layers.Dense(300, activation = 'relu')(input_actions)\n",
    "        #h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Concatenate()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        pred = tf.keras.layers.Dense(1, activation = 'sigmoid', \n",
    "                                     kernel_initializer=tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3))(h) \n",
    "        # or linear?\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)   \n",
    "        model.compile(loss='mse', optimizer=self.critic_opt)\n",
    "        self.critic_model = model\n",
    "\n",
    "        # create actor model:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(400, input_dim = self.input_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(300, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\", \n",
    "                                        kernel_initializer=tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)))\n",
    "        model.compile(loss = self.actor_loss, optimizer=self.actor_opt)\n",
    "        self.actor_model = model\n",
    "\n",
    "        # target models:\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    " \n",
    "    def sample_action(self, obs):\n",
    "        action = self.actor_model.predict(obs)[0]*self.action_range\n",
    "        action = action + self.noise_scale*np.random.normal(size = self.action_dim)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        return action\n",
    "           \n",
    "    \n",
    "    def store_info(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1e6:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-int(0.9e6):]\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else:            \n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)        \n",
    "        \n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size:\n",
    "            # get batch\n",
    "            idxs = np.random.randint(self.memory[\"done\"].shape[0], size=self.batch_size)\n",
    "            batch = {\n",
    "                \"state\": np.squeeze(self.memory[\"state\"][idxs]), \n",
    "                \"action\": np.squeeze(self.memory[\"action\"][idxs]), \n",
    "                \"reward\": self.memory[\"reward\"][idxs], \n",
    "                \"new_state\": np.squeeze(self.memory[\"new_state\"][idxs]), \n",
    "                \"done\": self.memory[\"done\"][idxs]\n",
    "            }\n",
    "            \n",
    "            # replay:\n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.action_range*self.target_actor_model(batch[\"new_state\"])})\n",
    "\n",
    "            y = batch[\"reward\"].reshape(self.batch_size,1) + np.multiply(target_q, \n",
    "                                                                        (self.discount_rate*(1-batch[\"done\"])\n",
    "                                                                        ).reshape(self.batch_size,1)\n",
    "                                                                        ) \n",
    "            \n",
    "            # update critic\n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": batch[\"action\"]}, training = True)\n",
    "                td_error = tf.losses.mean_squared_error(y, q)\n",
    "            critic_grads = tape.gradient(td_error, self.critic_model.trainable_weights)\n",
    "            self.critic_opt.apply_gradients(zip(critic_grads, self.critic_model.trainable_weights))\n",
    "\n",
    "            \n",
    "            # update actor policy\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"], training = True)\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred}, training = False)\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "\n",
    "                        \n",
    "            # update networks\n",
    "            self.critic_model.set_weights(self.tau * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          (1.0-self.tau)*np.array(self.critic_model.get_weights()))\n",
    "            self.actor_model.set_weights(self.tau * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         (1.0-self.tau)*np.array(self.actor_model.get_weights()))\n",
    "            \n",
    "            self.noise_scale*=0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent):\n",
    "    agent.critic_model.save('critic_model.h5')\n",
    "    agent.actor_model.save('actor_model.h5')\n",
    "    agent.target_critic_model.save('target_critic_model.h5')\n",
    "    agent.target_actor_model.save('target_actor_model.h5')\n",
    "    \n",
    "def train_model(env, n_iterations, batch_size, discount_rate):\n",
    "    scores = np.array([])\n",
    "    last_rewards = np.array([])\n",
    "    first_rewards = np.array([])\n",
    "    action_spec = env.action_spec()\n",
    "    agent = Agent(action_spec)\n",
    "    start = time.time()\n",
    "    for iteration in range(n_iterations):       \n",
    "        if iteration % 25 == 0:\n",
    "            save_models(agent)\n",
    "            print(\"{:10.3f} minutes remaining\".format((time.time()-start)/60))\n",
    "\n",
    "        time_step = env.reset()  \n",
    "        obs = convert_observation(time_step.observation)\n",
    "\n",
    "        game_score = 0\n",
    "        while not time_step.last():       \n",
    "            # get action:\n",
    "            action = agent.sample_action(obs)\n",
    "            # make action:\n",
    "            time_step = env.step(action)\n",
    "            new_obs = convert_observation(time_step.observation)            \n",
    "            # update history:\n",
    "            game_score += time_step.reward \n",
    "            agent.store_info(obs, new_obs, action, time_step.reward, 0)\n",
    "            obs = new_obs\n",
    "            # experience replay:\n",
    "            agent.experience_replay()\n",
    "            if len(first_rewards)==len(last_rewards):\n",
    "                first_rewards = np.append(first_rewards, time_step.reward)\n",
    "\n",
    "        new_obs = convert_observation(time_step.observation)   \n",
    "        agent.store_info(obs, new_obs, action, time_step.reward, 1)\n",
    "        agent.experience_replay()\n",
    "        game_score += time_step.reward \n",
    "        \n",
    "        scores = np.append(scores, game_score)\n",
    "        last_rewards = np.append(last_rewards, time_step.reward)\n",
    "        print(\"Iteration: {};   score: {:10.3f}; last_reward: {:10.3f}; first_reward: {:10.3f}\".format(\n",
    "            iteration, game_score, time_step.reward, first_rewards[-1]))\n",
    "\n",
    "    save_models(agent)\n",
    "    return scores, last_rewards, first_rewards, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.006 minutes remaining\n",
      "Iteration: 0;   score:     41.541; last_reward:      0.040; first_reward:      0.042\n",
      "Iteration: 1;   score:     26.095; last_reward:      0.026; first_reward:      0.032\n",
      "Iteration: 2;   score:    946.401; last_reward:      0.948; first_reward:      1.000\n",
      "Iteration: 3;   score:     13.442; last_reward:      0.014; first_reward:      0.011\n",
      "Iteration: 4;   score:    375.662; last_reward:      0.348; first_reward:      0.686\n",
      "Iteration: 5;   score:    901.306; last_reward:      0.914; first_reward:      1.000\n",
      "Iteration: 6;   score:    962.862; last_reward:      0.953; first_reward:      0.988\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 300\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")\n",
    "action_spec = env.action_spec()\n",
    "\n",
    "scores, last_rewards, first_rewards, agent = train_model(env, n_iterations, batch_size=64, discount_rate=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_arr = np.array((last_rewards[-100:] - first_rewards[-100:])>0, dtype = int)\n",
    "plt.figure(figsize=(10,7))\n",
    "better_results = np.sum(better_arr)\n",
    "plt.bar([0, 1],[len(better_arr)-better_results, better_results], 0.3)\n",
    "plt.xticks([0,1],labels = [\"False\", \"True\"])\n",
    "plt.title(\"Learning curves: is the last point is closer to the target than the first point\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.xlabel(\"Is closer\")\n",
    "plt.savefig(\"closer_points.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "x=list(range(len(scores)))\n",
    "plt.plot(x, scores)\n",
    "\n",
    "plt.title(\"Learning curves: score per iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.savefig(\"learning_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,2):\n",
    "    print(i,\"Average learning score on 100 steps: \", np.mean(scores[(i-1)*100: i*100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_games = 100\n",
    "\n",
    "test_scores = []\n",
    "test_last_rewards = []\n",
    "for game_index in range(test_games):\n",
    "    score = 0\n",
    "    time_step = env.reset()  \n",
    "    obs = convert_observation(time_step.observation)\n",
    "    while not time_step.last(): \n",
    "        action = agent.target_actor_model.predict(obs)\n",
    "        time_step = env.step(action[0])\n",
    "        obs = convert_observation(time_step.observation)\n",
    "        score += time_step.reward\n",
    "\n",
    "    test_scores.append(score)\n",
    "    test_last_rewards.append(time_step.reward)\n",
    "\n",
    "print(\"Average reward on test 100 games: \", np.mean(test_scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Reward on test 100 games')\n",
    "ax.boxplot(test_scores,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward\")\n",
    "plt.savefig(\"rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Rewards on the last step on test 100 games')\n",
    "ax.boxplot(test_last_rewards,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward on the last step\")\n",
    "plt.savefig(\"last_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
