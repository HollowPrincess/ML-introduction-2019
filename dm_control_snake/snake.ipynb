{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    list1 = [i.tolist() for i in list(obs.values())]\n",
    "    list2 = np.array([])\n",
    "    for sublist in list1:\n",
    "        list2 = np.append(list2, sublist)\n",
    "    return list2.reshape([1,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1801.00690.pdf\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-3, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, action_spec):\n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "        }\n",
    "        self.batch_size=64\n",
    "        self.discount_rate=0.99\n",
    "        self.tau=0.001\n",
    "        self.action_dim = len(action_spec.minimum) #action_spec.shape\n",
    "        self.action_spec = action_spec\n",
    "        self.action_bound = 1\n",
    "        self.input_size = 25\n",
    "        self.noise = 0\n",
    "        \n",
    "        self.critic_opt = tf.optimizers.Adam(1e-4)\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-4)\n",
    "        \n",
    "\n",
    "        # create critic model:\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(400, activation = 'relu')(input_obs)\n",
    "        tmp1 = tf.keras.layers.Dense(300, activation = 'relu')(h)\n",
    "        action_abs = tf.keras.layers.Dense(300, activation = 'relu')(input_actions)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        pred = tf.keras.layers.Dense(1, activation = 'relu')(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)   \n",
    "        model.compile(loss='mse', optimizer=self.critic_opt)\n",
    "        self.critic_model = model\n",
    "\n",
    "        \"\"\"\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(64, activation = 'relu')(input_obs)\n",
    "        #h = BatchNormalization()(h)\n",
    "        tmp1 = tf.keras.layers.Dense(64)(h)\n",
    "        action_abs = tf.keras.layers.Dense(64, activation = 'relu')(input_actions)\n",
    "        #action_abs = Activation('relu')(action_abs)\n",
    "        #action_abs = BatchNormalization()(action_abs)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        #h = Dense(64)(h)\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        #h = BatchNormalization()(h)\n",
    "        pred = tf.keras.layers.Dense(1, kernel_initializer='random_uniform')(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        self.critic_model = model\n",
    "        \"\"\"\n",
    "        # create actor model:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(300, input_dim = self.input_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\"))\n",
    "        #model.add(tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1, 1)))\n",
    "        model.compile(loss='mse', optimizer=self.actor_opt)\n",
    "        self.actor_model = model\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(400, input_dim = self.input_size))#, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(300))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\"))\n",
    "        model.compile(loss='mse', optimizer=\"Adam\")\n",
    "        self.actor_model = model\n",
    "        \"\"\"\n",
    "        # target models:\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    " \n",
    "    def sample_action(self, obs):\n",
    "        action = self.actor_model.predict(obs)\n",
    "        action = action + self.noise()\n",
    "        #0.3*np.random.normal(0, 1, self.action_spec.shape) #2*self.noise()# np.random.normal(0, 0.3, self.action_spec.shape) # noise scaled from -1 to 1 \n",
    "        action = np.clip(action, -1, 1)\n",
    "        return action\n",
    "    \n",
    "    def init_noise(self):\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(mu = np.zeros(self.action_dim))\n",
    "    \n",
    "    def store_info(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1e6:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-int(0.9e6):]\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else:            \n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)        \n",
    "        \n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size:\n",
    "            # get batch\n",
    "            idxs = np.random.randint(self.memory[\"done\"].shape[0], size=self.batch_size)\n",
    "            batch = {\n",
    "                \"state\": np.squeeze(self.memory[\"state\"][idxs]), \n",
    "                \"action\": np.squeeze(self.memory[\"action\"][idxs]), \n",
    "                \"reward\": self.memory[\"reward\"][idxs], \n",
    "                \"new_state\": np.squeeze(self.memory[\"new_state\"][idxs]), \n",
    "                \"done\": self.memory[\"done\"][idxs]\n",
    "            }\n",
    "            \n",
    "            # replay:\n",
    "            #target_q = self.target_critic_model.predict_on_batch({\"state\":batch[\"new_state\"], \n",
    "            #                                \"action\": self.target_actor_model.predict_on_batch(batch[\"new_state\"])})\n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.target_actor_model(batch[\"new_state\"])})\n",
    "\n",
    "            #y = batch[\"reward\"].reshape(self.batch_size,1) + self.discount_rate*target_q\n",
    "            y = batch[\"reward\"].reshape(self.batch_size,1) + np.multiply(target_q, \n",
    "                                                                        (self.discount_rate*(1-batch[\"done\"])\n",
    "                                                                        ).reshape(self.batch_size,1)\n",
    "                                                                        ) \n",
    "            \n",
    "            # update critic\n",
    "            #self.critic_model.train_on_batch({\"state\": batch[\"state\"], \"action\": batch[\"action\"]}, y)  \n",
    "            # trainable weights or variables?\n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": batch[\"action\"]})\n",
    "                td_error = tf.losses.mean_squared_error(y, q)\n",
    "            critic_grads = tape.gradient(td_error, self.critic_model.trainable_variables)\n",
    "            self.critic_opt.apply_gradients(zip(critic_grads, self.critic_model.trainable_variables))\n",
    "                \n",
    "            \n",
    "            # update actor policy\n",
    "            #actions_pred = self.actor_model.predict_on_batch(batch[\"state\"])\n",
    "            #critic_pred = self.critic_model.predict_on_batch({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "            \"\"\"            \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                tape.watch(actions_pred)\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "            dq_da = tape.gradient(q, actions_pred)\n",
    "            \n",
    "            self.actor_model.train_on_batch(batch[\"state\"], dq_da)\n",
    "            \"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                theta = self.actor_model.trainable_variables\n",
    "            da_dtheta = tape.gradient(actions_pred, theta, output_gradients=-dq_da)\n",
    "            \n",
    "            self.actor_opt.apply_gradients(zip(da_dtheta, \n",
    "                                               self.actor_model.trainable_variables))\n",
    "            \"\"\"\n",
    "            #self.actor_model.train_on_batch(batch[\"state\"], critic_pred)    \n",
    "            \n",
    "            # update networks\n",
    "            self.critic_model.set_weights(self.tau * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          (1.0-self.tau)*np.array(self.critic_model.get_weights()))\n",
    "            self.actor_model.set_weights(self.tau * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         (1.0-self.tau)*np.array(self.actor_model.get_weights()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent):\n",
    "    agent.critic_model.save('critic_model.h5')\n",
    "    agent.actor_model.save('actor_model.h5')\n",
    "    agent.target_critic_model.save('critic_model.h5')\n",
    "    agent.target_actor_model.save('actor_model.h5')\n",
    "    \n",
    "def train_model(env, n_iterations, batch_size, discount_rate):\n",
    "    scores = np.array([])\n",
    "    last_rewards = np.array([])\n",
    "    first_rewards = np.array([])\n",
    "    action_spec = env.action_spec()\n",
    "    agent = Agent(action_spec)\n",
    "    start = time.time()\n",
    "    for iteration in range(n_iterations):\n",
    "        agent.init_noise()\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            save_models(agent)\n",
    "\n",
    "        time_step = env.reset()  \n",
    "        obs = convert_observation(time_step.observation)\n",
    "\n",
    "        game_score = 0\n",
    "        while not time_step.last():       \n",
    "            # get action:\n",
    "            action = agent.sample_action(obs)\n",
    "            # make action:\n",
    "            time_step = env.step(action[0])\n",
    "            new_obs = convert_observation(time_step.observation)            \n",
    "            # update history:\n",
    "            game_score += time_step.reward \n",
    "            agent.store_info(obs, new_obs, action, time_step.reward, 0)\n",
    "            obs = new_obs\n",
    "            # experience replay:\n",
    "            agent.experience_replay()\n",
    "            if len(first_rewards)==len(last_rewards):\n",
    "                first_rewards = np.append(first_rewards, time_step.reward)\n",
    "\n",
    "        new_obs = convert_observation(time_step.observation)   \n",
    "        agent.store_info(obs, new_obs, action, time_step.reward, 1)\n",
    "        agent.experience_replay()\n",
    "        game_score += time_step.reward \n",
    "        \n",
    "        scores = np.append(scores, game_score)\n",
    "        last_rewards = np.append(last_rewards, time_step.reward)\n",
    "        print(\"Iteration: {}; score: {:10.3f}; last_reward: {:10.3f}; first_reward: {:10.3f}\".format(\n",
    "            iteration, game_score, time_step.reward, first_rewards[-1]))\n",
    "        print(\"{:10.3f} minutes remaining\".format((time.time()-start)/60))\n",
    "\n",
    "    save_models(agent)\n",
    "    return scores, last_rewards, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; score:    267.701; last_reward:      0.391; first_reward:      0.137\n",
      "     1.005 minutes remaining\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 400\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")\n",
    "action_spec = env.action_spec()\n",
    "\n",
    "scores, last_rewards, agent = train_model(env, n_iterations, batch_size=64, discount_rate=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores[-15:])\n",
    "print(last_rewards[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "x=list(range(len(scores)))\n",
    "plt.plot(x, scores)\n",
    "\n",
    "plt.title(\"Learning curves: score per iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.savefig(\"learning_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_games = 100\n",
    "\n",
    "test_scores = []\n",
    "test_last_rewards = []\n",
    "for game_index in range(test_games):\n",
    "    score = 0\n",
    "    time_step = env.reset()  \n",
    "    obs = convert_observation(time_step.observation)\n",
    "    while not time_step.last(): \n",
    "        action = agent.target_actor_model.predict(obs)\n",
    "        time_step = env.step(action[0])\n",
    "        obs = convert_observation(time_step.observation)\n",
    "        score += time_step.reward\n",
    "\n",
    "    test_scores.append(score)\n",
    "    test_last_rewards.append(time_step.reward)\n",
    "\n",
    "print(\"Average reward on test 100 games: \", np.mean(test_scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Reward on test 100 games')\n",
    "ax.boxplot(test_scores,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward\")\n",
    "plt.savefig(\"rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Rewards on the last step on test 100 games')\n",
    "ax.boxplot(test_last_rewards,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward on the last step\")\n",
    "plt.savefig(\"last_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
