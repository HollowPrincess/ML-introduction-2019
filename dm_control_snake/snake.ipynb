{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_observation(obs):\n",
    "    list1 = [i.tolist() for i in list(obs.values())]\n",
    "    list2 = np.array([])\n",
    "    for sublist in list1:\n",
    "        list2 = np.append(list2, sublist)\n",
    "    return list2.reshape([1,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-3, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, action_spec):\n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "        }\n",
    "        self.batch_size=64\n",
    "        self.discount_rate=0.99\n",
    "        self.tau=0.001\n",
    "        self.action_dim = len(action_spec.minimum) #action_spec.shape\n",
    "        self.action_spec = action_spec\n",
    "        self.action_bound = 1\n",
    "        self.input_size = 25\n",
    "        self.noise = 0\n",
    "        \n",
    "        self.critic_opt = tf.optimizers.Adam(1e-4)\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-4)\n",
    "        \n",
    "\n",
    "        # create critic model:\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(400, activation = 'relu', W_init=tf.keras.initializers.glorot_normal())(input_obs)\n",
    "        tmp1 = tf.keras.layers.Dense(300, activation = 'relu', W_init=tf.keras.initializers.glorot_normal())(h)\n",
    "        action_abs = tf.keras.layers.Dense(300, activation = 'relu', W_init=tf.keras.initializers.glorot_normal()\n",
    "                                          )(input_actions)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        pred = tf.keras.layers.Dense(1, W_init=tf.keras.initializers.glorot_normal())(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)   \n",
    "        model.compile(loss='mse', optimizer=self.critic_opt)\n",
    "        self.critic_model = model\n",
    "\n",
    "        \"\"\"\n",
    "        input_obs = tf.keras.Input(shape=self.input_size, name = 'state')\n",
    "        input_actions = tf.keras.Input(shape=(self.action_dim,), name = 'action')\n",
    "        h = tf.keras.layers.Dense(64, activation = 'relu')(input_obs)\n",
    "        #h = BatchNormalization()(h)\n",
    "        tmp1 = tf.keras.layers.Dense(64)(h)\n",
    "        action_abs = tf.keras.layers.Dense(64, activation = 'relu')(input_actions)\n",
    "        #action_abs = Activation('relu')(action_abs)\n",
    "        #action_abs = BatchNormalization()(action_abs)\n",
    "        h = tf.keras.layers.Add()([tmp1,action_abs])\n",
    "        #h = Dense(64)(h)\n",
    "        h = tf.keras.layers.Activation('relu')(h)\n",
    "        #h = BatchNormalization()(h)\n",
    "        pred = tf.keras.layers.Dense(1, kernel_initializer='random_uniform')(h)\n",
    "        model = tf.keras.Model(inputs=[input_obs, input_actions], outputs=pred)\n",
    "        model.compile(loss='mse', optimizer='Adam')\n",
    "        self.critic_model = model\n",
    "        \"\"\"\n",
    "        # create actor model:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(300, input_dim = self.input_size, activation='relu', \n",
    "                                        W_init=tf.keras.initializers.glorot_normal()))\n",
    "        model.add(tf.keras.layers.Dense(200, activation='relu',\n",
    "                                       W_init=tf.keras.initializers.glorot_normal()))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\",\n",
    "                                       W_init=tf.keras.initializers.glorot_normal()))\n",
    "        model.add(tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1, 1)))\n",
    "        model.compile(loss='mse', optimizer=self.actor_opt)\n",
    "        self.actor_model = model\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(400, input_dim = self.input_size))#, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(300))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Activation('relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_dim, activation = \"tanh\"))\n",
    "        model.compile(loss='mse', optimizer=\"Adam\")\n",
    "        self.actor_model = model\n",
    "        \"\"\"\n",
    "        # target models:\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    " \n",
    "    def sample_action(self, obs):\n",
    "        action = self.actor_model.predict(obs)\n",
    "        action = action + 0.1*np.random.normal(0, 1, self.action_spec.shape) #2*self.noise()# np.random.normal(0, 0.3, self.action_spec.shape) # noise scaled from -1 to 1 \n",
    "        action = np.clip(action, -1, 1)\n",
    "        return action\n",
    "    \n",
    "    def init_noise(self):\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(mu = np.zeros(self.action_dim))\n",
    "    \n",
    "    def store_info(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1e6:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-int(0.9e6):]\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else:            \n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)        \n",
    "        \n",
    "        \n",
    "    def experience_replay(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size:\n",
    "            # get batch\n",
    "            idxs = np.random.randint(self.memory[\"done\"].shape[0], size=self.batch_size)\n",
    "            batch = {\n",
    "                \"state\": np.squeeze(self.memory[\"state\"][idxs]), \n",
    "                \"action\": np.squeeze(self.memory[\"action\"][idxs]), \n",
    "                \"reward\": self.memory[\"reward\"][idxs], \n",
    "                \"new_state\": np.squeeze(self.memory[\"new_state\"][idxs]), \n",
    "                \"done\": self.memory[\"done\"][idxs]\n",
    "            }\n",
    "            \n",
    "            # replay:\n",
    "            #target_q = self.target_critic_model.predict_on_batch({\"state\":batch[\"new_state\"], \n",
    "            #                                \"action\": self.target_actor_model.predict_on_batch(batch[\"new_state\"])})\n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.target_actor_model(batch[\"new_state\"])})\n",
    "\n",
    "            #y = batch[\"reward\"].reshape(self.batch_size,1) + self.discount_rate*target_q\n",
    "            y = batch[\"reward\"].reshape(self.batch_size,1) + np.multiply(target_q, \n",
    "                                                                        (self.discount_rate*(1-batch[\"done\"])\n",
    "                                                                        ).reshape(self.batch_size,1)\n",
    "                                                                        ) \n",
    "            \n",
    "            # update critic\n",
    "            #self.critic_model.train_on_batch({\"state\": batch[\"state\"], \"action\": batch[\"action\"]}, y)  \n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": batch[\"action\"]})\n",
    "                td_error = tf.losses.mean_squared_error(y, q)\n",
    "            critic_grads = tape.gradient(td_error, self.critic_model.trainable_weights)\n",
    "            self.critic_opt.apply_gradients(zip(critic_grads, self.critic_model.trainable_weights))\n",
    "                \n",
    "            \n",
    "            # update actor policy\n",
    "            #actions_pred = self.actor_model.predict_on_batch(batch[\"state\"])\n",
    "            #critic_pred = self.critic_model.predict_on_batch({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "            with tf.GradientTape() as tape:\n",
    "                actions_pred = self.actor_model(batch[\"state\"])\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], \"action\": actions_pred})\n",
    "                actor_loss = - tf.reduce_mean(q)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "            #self.actor_model.train_on_batch(batch[\"state\"], critic_pred)    \n",
    "            \n",
    "            # update networks\n",
    "            self.critic_model.set_weights(self.tau * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          (1.0-self.tau)*np.array(self.critic_model.get_weights()))\n",
    "            self.actor_model.set_weights(self.tau * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         (1.0-self.tau)*np.array(self.actor_model.get_weights()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent):\n",
    "    agent.critic_model.save('critic_model.h5')\n",
    "    agent.actor_model.save('actor_model.h5')\n",
    "    agent.target_critic_model.save('critic_model.h5')\n",
    "    agent.target_actor_model.save('actor_model.h5')\n",
    "    \n",
    "def train_model(env, n_iterations, batch_size, discount_rate):\n",
    "    scores = np.array([])\n",
    "    last_rewards = np.array([])\n",
    "    first_rewards = np.array([])\n",
    "    action_spec = env.action_spec()\n",
    "    agent = Agent(action_spec)\n",
    "    start = time.time()\n",
    "    for iteration in range(n_iterations):\n",
    "        agent.init_noise()\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            save_models(agent)\n",
    "\n",
    "        time_step = env.reset()  \n",
    "        obs = convert_observation(time_step.observation)\n",
    "\n",
    "        game_score = 0\n",
    "        while not time_step.last():       \n",
    "            # get action:\n",
    "            action = agent.sample_action(obs)\n",
    "            # make action:\n",
    "            time_step = env.step(action[0])\n",
    "            new_obs = convert_observation(time_step.observation)            \n",
    "            # update history:\n",
    "            game_score += time_step.reward \n",
    "            agent.store_info(obs, new_obs, action, time_step.reward, 0)\n",
    "            obs = new_obs\n",
    "            # experience replay:\n",
    "            agent.experience_replay()\n",
    "            if len(first_rewards)==len(last_rewards):\n",
    "                first_rewards = np.append(first_rewards, time_step.reward)\n",
    "\n",
    "        new_obs = convert_observation(time_step.observation)   \n",
    "        agent.store_info(obs, new_obs, action, time_step.reward, 1)\n",
    "        agent.experience_replay()\n",
    "        game_score += time_step.reward \n",
    "        \n",
    "        scores = np.append(scores, game_score)\n",
    "        last_rewards = np.append(last_rewards, time_step.reward)\n",
    "        print(\"Iteration: {}; score: {:10.3f}; last_reward: {:10.3f}; first_reward: {:10.3f}\".format(\n",
    "            iteration, game_score, time_step.reward, first_rewards[-1]))\n",
    "        print(\"{:10.3f} minutes remaining\".format((time.time()-start)/60))\n",
    "\n",
    "    save_models(agent)\n",
    "    return scores, last_rewards, agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0; score:     17.885; last_reward:      0.022; first_reward:      0.016\n",
      "     1.025 minutes remaining\n",
      "Iteration: 1; score:    535.337; last_reward:      0.506; first_reward:      0.380\n",
      "     2.098 minutes remaining\n",
      "Iteration: 2; score:     12.737; last_reward:      0.014; first_reward:      0.010\n",
      "     3.170 minutes remaining\n",
      "Iteration: 3; score:     17.691; last_reward:      0.017; first_reward:      0.021\n",
      "     4.193 minutes remaining\n",
      "Iteration: 4; score:     27.361; last_reward:      0.024; first_reward:      0.027\n",
      "     5.200 minutes remaining\n",
      "Iteration: 5; score:     20.472; last_reward:      0.020; first_reward:      0.029\n",
      "     6.223 minutes remaining\n",
      "Iteration: 6; score:      7.794; last_reward:      0.008; first_reward:      0.007\n",
      "     7.276 minutes remaining\n",
      "Iteration: 7; score:      4.897; last_reward:      0.005; first_reward:      0.005\n",
      "     8.368 minutes remaining\n",
      "Iteration: 8; score:      4.462; last_reward:      0.005; first_reward:      0.004\n",
      "     9.485 minutes remaining\n",
      "Iteration: 9; score:      7.234; last_reward:      0.007; first_reward:      0.008\n",
      "    10.605 minutes remaining\n",
      "Iteration: 10; score:     35.214; last_reward:      0.034; first_reward:      0.031\n",
      "    11.762 minutes remaining\n",
      "Iteration: 11; score:     30.034; last_reward:      0.031; first_reward:      0.026\n",
      "    12.880 minutes remaining\n",
      "Iteration: 12; score:    738.238; last_reward:      0.765; first_reward:      0.686\n",
      "    14.022 minutes remaining\n",
      "Iteration: 13; score:     11.931; last_reward:      0.012; first_reward:      0.012\n",
      "    15.116 minutes remaining\n",
      "Iteration: 14; score:     20.432; last_reward:      0.020; first_reward:      0.023\n",
      "    16.244 minutes remaining\n",
      "Iteration: 15; score:     14.306; last_reward:      0.014; first_reward:      0.014\n",
      "    17.365 minutes remaining\n",
      "Iteration: 16; score:      7.626; last_reward:      0.008; first_reward:      0.008\n",
      "    18.521 minutes remaining\n",
      "Iteration: 17; score:    305.024; last_reward:      0.283; first_reward:      0.666\n",
      "    19.630 minutes remaining\n",
      "Iteration: 18; score:    749.711; last_reward:      0.729; first_reward:      0.566\n",
      "    20.775 minutes remaining\n",
      "Iteration: 19; score:     15.601; last_reward:      0.015; first_reward:      0.017\n",
      "    21.930 minutes remaining\n",
      "Iteration: 20; score:      6.905; last_reward:      0.007; first_reward:      0.006\n",
      "    23.060 minutes remaining\n",
      "Iteration: 21; score:     12.334; last_reward:      0.013; first_reward:      0.012\n",
      "    24.202 minutes remaining\n",
      "Iteration: 22; score:     35.822; last_reward:      0.036; first_reward:      0.045\n",
      "    25.345 minutes remaining\n",
      "Iteration: 23; score:    984.658; last_reward:      1.000; first_reward:      0.817\n",
      "    26.497 minutes remaining\n",
      "Iteration: 24; score:     10.818; last_reward:      0.011; first_reward:      0.013\n",
      "    27.629 minutes remaining\n",
      "Iteration: 25; score:      5.453; last_reward:      0.005; first_reward:      0.006\n",
      "    28.787 minutes remaining\n",
      "Iteration: 26; score:    867.611; last_reward:      0.936; first_reward:      0.475\n",
      "    29.985 minutes remaining\n",
      "Iteration: 27; score:    753.365; last_reward:      0.750; first_reward:      1.000\n",
      "    31.180 minutes remaining\n",
      "Iteration: 28; score:     43.813; last_reward:      0.045; first_reward:      0.051\n",
      "    32.340 minutes remaining\n",
      "Iteration: 29; score:     15.507; last_reward:      0.016; first_reward:      0.014\n",
      "    33.515 minutes remaining\n",
      "Iteration: 30; score:      5.952; last_reward:      0.006; first_reward:      0.006\n",
      "    34.702 minutes remaining\n",
      "Iteration: 31; score:      6.902; last_reward:      0.007; first_reward:      0.007\n",
      "    35.877 minutes remaining\n",
      "Iteration: 32; score:     17.978; last_reward:      0.019; first_reward:      0.016\n",
      "    37.099 minutes remaining\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1000\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")\n",
    "action_spec = env.action_spec()\n",
    "\n",
    "scores, last_rewards, agent = train_model(env, n_iterations, batch_size=64, discount_rate=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores[-15:])\n",
    "print(last_rewards[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "x=list(range(len(scores)))\n",
    "plt.plot(x, scores)\n",
    "\n",
    "plt.title(\"Learning curves: score per iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.xlabel(\"Iteration number\")\n",
    "plt.savefig(\"learning_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_games = 100\n",
    "\n",
    "test_scores = []\n",
    "test_last_rewards = []\n",
    "for game_index in range(test_games):\n",
    "    score = 0\n",
    "    time_step = env.reset()  \n",
    "    obs = convert_observation(time_step.observation)\n",
    "    while not time_step.last(): \n",
    "        action = agent.target_actor_model.predict(obs)\n",
    "        time_step = env.step(action[0])\n",
    "        obs = convert_observation(time_step.observation)\n",
    "        score += time_step.reward\n",
    "\n",
    "    test_scores.append(score)\n",
    "    test_last_rewards.append(time_step.reward)\n",
    "\n",
    "print(\"Average reward on test 100 games: \", np.mean(test_scores))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Reward on test 100 games')\n",
    "ax.boxplot(test_scores,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward\")\n",
    "plt.savefig(\"rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.set_title('Rewards on the last step on test 100 games')\n",
    "ax.boxplot(test_last_rewards,   \n",
    "          showfliers=True)\n",
    "\n",
    "ax.set_ylabel(\"Reward on the last step\")\n",
    "plt.savefig(\"last_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
