{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataframe preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/Dataset/Dataset/Training/Features_Variant_1.csv', header=None).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get and fit features names\n",
    "features_names=np.array(['page_likes_num',#1\n",
    "               'page_checkins',#2\n",
    "               'page_talking_about',#3\n",
    "                'page_cat',#4\n",
    "                'page_statistics',#5-29 #mean, avg etc.\n",
    "                'comments_num_before_base_time',#30\n",
    "                'comments_num_in_last_24_hours',#31 #last day\n",
    "                'comments_num_in_last_48_to_24_hours',#32 #day before last\n",
    "                'comments_num_in_first_24_hours',#33\n",
    "                'comments_difference_in_last_two_days', #34 (32-31)\n",
    "                'base_time', #35\n",
    "                'character_num_in_post', #36\n",
    "                'share_num',#37\n",
    "                'post_promotion', #38 binary\n",
    "                'h_local', #39 This describes the H hrs, for which we have the target variable/ comments received. \n",
    "                'post_published_weekday', #40-46 This represents the day(Sunday...Saturday) on which the post was published. \n",
    "                'base_ditetime_weekday', #47-53 This represents the day(Sunday...Saturday) on selected base Date/Time. \n",
    "                'target' #54 The no of comments in next H hrs(H is given in Feature no 39).                \n",
    "               ])\n",
    "\n",
    "for index in range(5,29):\n",
    "    features_names=np.insert(features_names, index, features_names[4]+'_'+str(index-4))\n",
    "    \n",
    "weekday=('sunday', 'monday','tuesday', 'wednesday', 'thursday', 'friday', 'saturday')    \n",
    "\n",
    "for index in range(40,47):\n",
    "    features_names=np.insert(features_names,index, features_names[39]+'_'+ weekday[index-40])\n",
    "features_names=np.delete(features_names, 39)\n",
    "\n",
    "for index in range(47,54):\n",
    "    features_names=np.insert(features_names,index, features_names[46]+'_'+ weekday[index-47])\n",
    "features_names=np.delete(features_names, 46)\n",
    "\n",
    "data.columns=features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_promotion                      1\n",
      "post_published_weekday_sunday       2\n",
      "post_published_weekday_monday       2\n",
      "post_published_weekday_tuesday      2\n",
      "post_published_weekday_wednesday    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y=data.nunique(axis=0).sort_values()\n",
    "print(y.head())\n",
    "data=data.drop(columns=['post_promotion']) #post promotion has only zero values we can drop this col because it isn't informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'page_cat' to dummy because it is a category\n",
    "data=pd.concat([data,pd.get_dummies(data['page_cat'], prefix='page_cat')], axis=1)\n",
    "data=data.drop(columns=['page_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data shuffling\n",
    "from sklearn.utils import shuffle\n",
    "data=shuffle(data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    std=np.std(df, ddof=1).replace(0, 1)\n",
    "    mean=np.mean(df)\n",
    "    return (df-mean)/std, mean, std\n",
    "\n",
    "def normalize_with_params(df, mean, std):\n",
    "    return (df-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_RMSE(target, pred, gamma, w):\n",
    "    return np.sqrt(\n",
    "        np.sum((target-pred)**2)/len(pred)+gamma*np.linalg.norm(w)\n",
    "    )\n",
    "\n",
    "def get_para_w(old_w, df, pred, lam, gamma, indices):\n",
    "    #indices=np.random.choice(df.shape[0]-1,terms_num,replace=False)\n",
    "    old_w=old_w+2*lam*(\n",
    "          (np.dot((df.loc[indices,'target']-pred[indices]).values,df.iloc[indices,:-1].values))/len(pred)\n",
    "        )\n",
    "    return old_w\n",
    "\n",
    "def get_para_w0(old_w0, df, pred, lam, gamma, indices):\n",
    "    return old_w0+2*lam*(\n",
    "        (np.sum(df.loc[indices,'target']-pred[indices]))/len(pred))\n",
    "\n",
    "def get_prediction(w,w0,x_df):\n",
    "    return np.dot(x_df, w)+w0\n",
    "    \n",
    "def gradient_descend(df, lam, gamma, terms_num, max_iter):\n",
    "    df=df.reset_index(drop=True)\n",
    "    w=np.array([0]*(df.shape[1]-1), dtype=float) #one column is the target\n",
    "    w0=0.0\n",
    "    prediction=get_prediction(w,w0,df.iloc[:,:-1])\n",
    "    minRMSE=custom_RMSE(df['target'], prediction, gamma, w)\n",
    "\n",
    "    best_params=np.append(w,w0)\n",
    "    best_pred=prediction\n",
    "\n",
    "    curr_err=10\n",
    "    err=1e-4\n",
    "    \n",
    "    iter_num=0\n",
    "    while (curr_err>err) & (iter_num<max_iter) :    \n",
    "        iter_num+=1\n",
    "        df=shuffle(df)\n",
    "        prediction=get_prediction(w,w0,df.iloc[:,:-1])\n",
    "        curr_err=prediction.copy()\n",
    "        \n",
    "        for batch_counter in range(0, math.ceil(df.shape[0]/terms_num)):\n",
    "            indices=np.array(df.iloc[batch_counter*terms_num:(batch_counter+1)*terms_num].index)            \n",
    "            w=get_para_w(w, df, prediction, lam, gamma, indices)\n",
    "            w0=get_para_w0(w0, df, prediction, lam, gamma, indices)            \n",
    "            prediction=get_prediction(w,w0,df.iloc[:,:-1])\n",
    "\n",
    "        if custom_RMSE(df['target'], prediction, gamma, w)<minRMSE:\n",
    "            minRMSE=custom_RMSE(df['target'], prediction, gamma, w)\n",
    "            best_params=np.append(w,w0)\n",
    "            best_pred=prediction\n",
    "        curr_err=np.linalg.norm(curr_err-prediction)\n",
    "            \n",
    "    print('Iterations number is:' +str(iter_num))\n",
    "    if iter_num==max_iter:\n",
    "        print('The maximum number of iterations was reached.')\n",
    "    print('RMSE is:'+str(minRMSE))\n",
    "    print('R2 is:'+str(custom_R2(df.iloc[:,-1], best_pred)))\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set algo params:\n",
    "lam=1e-2\n",
    "gamma=0 #now we needn't it\n",
    "terms_num=1000\n",
    "max_iter=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_R2(target, pred):\n",
    "    return 1-(np.sum((target-pred)**2))/(np.sum((target-np.mean(target))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risam\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations number is:30\n",
      "The maximum number of iterations was reached.\n",
      "RMSE is:0.9990914222811653\n",
      "R2 is:-0.007900518709232252\n",
      "Iterations number is:30\n",
      "The maximum number of iterations was reached.\n",
      "RMSE is:0.998360282633384\n",
      "R2 is:-0.021417169372734968\n",
      "Iterations number is:30\n",
      "The maximum number of iterations was reached.\n",
      "RMSE is:0.9981234541143703\n",
      "R2 is:-0.01713610991233927\n",
      "Iterations number is:30\n",
      "The maximum number of iterations was reached.\n",
      "RMSE is:0.9986843409831754\n",
      "R2 is:-0.022959305636945704\n"
     ]
    }
   ],
   "source": [
    "res=pd.DataFrame()\n",
    "\n",
    "parts=range(1,6)\n",
    "idxs=[round(data.shape[0]/5)*(i-1) for i in range(1,6)]\n",
    "idxs.append(data.shape[0])\n",
    "\n",
    "for part in parts:\n",
    "    #separate dataset\n",
    "    test_part=data[idxs[part-1]:idxs[part]]\n",
    "    train_part=data.drop(data.index[idxs[part-1]:idxs[part]])\n",
    "    #features normalization (xi-mean)/std_err:\n",
    "    train_part.iloc[:,:-1], mean, std=normalize(train_part.iloc[:,:-1])#fit and transform train\n",
    "    test_part.iloc[:,:-1]=normalize_with_params(test_part.iloc[:,:-1], mean, std) #transform test\n",
    "    params=gradient_descend(train_part, lam, gamma, terms_num, max_iter) #model    \n",
    "    test_pred=get_prediction(params[:-1],params[-1],test_part.iloc[:,:-1])\n",
    "    RMSE=custom_RMSE(test_part['target'], test_pred, gamma, params[:-1])\n",
    "    r2=custom_R2(test_part['target'], test_pred)\n",
    "    params=np.insert(params, 0 , [RMSE,r2])\n",
    "    res[part]=params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idxs=np.array(['w'+str(i) for i in range(1,res.shape[0]-2)])\n",
    "text_idxs=np.append(text_idxs,'w0')\n",
    "text_idxs=np.insert(text_idxs, 0, ['RMSE', 'r2'])\n",
    "res.index=text_idxs\n",
    "\n",
    "res['mean']=np.mean(res, axis=1)\n",
    "res['std']=np.std(res.iloc[:,:-1], ddof=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
